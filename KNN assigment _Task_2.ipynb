{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe34095c",
   "metadata": {},
   "source": [
    "**KNN from Scratch**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9ad06b",
   "metadata": {},
   "source": [
    "**Step 1: Load the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29630286",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Shape: (53940, 10)\n",
      "\n",
      "First few rows:\n",
      "   carat      cut color clarity  depth  table  price     x     y     z\n",
      "0   0.23    Ideal     E     SI2   61.5   55.0    326  3.95  3.98  2.43\n",
      "1   0.21  Premium     E     SI1   59.8   61.0    326  3.89  3.84  2.31\n",
      "2   0.23     Good     E     VS1   56.9   65.0    327  4.05  4.07  2.31\n",
      "3   0.29  Premium     I     VS2   62.4   58.0    334  4.20  4.23  2.63\n",
      "4   0.31     Good     J     SI2   63.3   58.0    335  4.34  4.35  2.75\n",
      "\n",
      "Dataset Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 53940 entries, 0 to 53939\n",
      "Data columns (total 10 columns):\n",
      " #   Column   Non-Null Count  Dtype  \n",
      "---  ------   --------------  -----  \n",
      " 0   carat    53940 non-null  float64\n",
      " 1   cut      53940 non-null  object \n",
      " 2   color    53940 non-null  object \n",
      " 3   clarity  53940 non-null  object \n",
      " 4   depth    53940 non-null  float64\n",
      " 5   table    53940 non-null  float64\n",
      " 6   price    53940 non-null  int64  \n",
      " 7   x        53940 non-null  float64\n",
      " 8   y        53940 non-null  float64\n",
      " 9   z        53940 non-null  float64\n",
      "dtypes: float64(6), int64(1), object(3)\n",
      "memory usage: 4.1+ MB\n",
      "None\n",
      "\n",
      "Basic Statistics:\n",
      "              carat         depth         table         price             x  \\\n",
      "count  53940.000000  53940.000000  53940.000000  53940.000000  53940.000000   \n",
      "mean       0.797940     61.749405     57.457184   3932.799722      5.731157   \n",
      "std        0.474011      1.432621      2.234491   3989.439738      1.121761   \n",
      "min        0.200000     43.000000     43.000000    326.000000      0.000000   \n",
      "25%        0.400000     61.000000     56.000000    950.000000      4.710000   \n",
      "50%        0.700000     61.800000     57.000000   2401.000000      5.700000   \n",
      "75%        1.040000     62.500000     59.000000   5324.250000      6.540000   \n",
      "max        5.010000     79.000000     95.000000  18823.000000     10.740000   \n",
      "\n",
      "                  y             z  \n",
      "count  53940.000000  53940.000000  \n",
      "mean       5.734526      3.538734  \n",
      "std        1.142135      0.705699  \n",
      "min        0.000000      0.000000  \n",
      "25%        4.720000      2.910000  \n",
      "50%        5.710000      3.530000  \n",
      "75%        6.540000      4.040000  \n",
      "max       58.900000     31.800000  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the diamonds dataset\n",
    "file_path = r\"C:\\Users\\DELL\\Downloads\\diamonds (1).csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(\"Dataset Shape:\", df.shape)\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(df.head())\n",
    "print(\"\\nDataset Info:\")\n",
    "print(df.info())\n",
    "print(\"\\nBasic Statistics:\")\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7a8cad",
   "metadata": {},
   "source": [
    "**Step 2: Identify input and output variables**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7962b40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features shape: (53940, 9)\n",
      "Target shape: (53940,)\n",
      "\n",
      "Feature columns: ['carat', 'cut', 'color', 'clarity', 'depth', 'table', 'x', 'y', 'z']\n"
     ]
    }
   ],
   "source": [
    "# Target variable: price\n",
    "# Features: all columns except price\n",
    "\n",
    "X = df.drop('price', axis=1)\n",
    "y = df['price']\n",
    "\n",
    "print(\"Features shape:\", X.shape)\n",
    "print(\"Target shape:\", y.shape)\n",
    "print(\"\\nFeature columns:\", X.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb60d6c",
   "metadata": {},
   "source": [
    "**Step 3: Split the data (75:25 split)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b49c01a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (40455, 9)\n",
      "Test set shape: (13485, 9)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=42\n",
    ")\n",
    "\n",
    "print(\"Training set shape:\", X_train.shape)\n",
    "print(\"Test set shape:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63b46e3",
   "metadata": {},
   "source": [
    "**Step 4: Data Preprocessing on X_train**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24a3b264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical columns: ['cut', 'color', 'clarity']\n",
      "Numeric columns: ['carat', 'depth', 'table', 'x', 'y', 'z']\n",
      "\n",
      "Processed training data shape: (40455, 26)\n",
      "Type: <class 'numpy.ndarray'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:972: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Identify column types\n",
    "categorical_cols = X_train.select_dtypes(include=['object']).columns.tolist()\n",
    "numeric_cols = X_train.select_dtypes(exclude=['object']).columns.tolist()\n",
    "\n",
    "print(\"Categorical columns:\", categorical_cols)\n",
    "print(\"Numeric columns:\", numeric_cols)\n",
    "\n",
    "# Create preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_cols),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse=False), categorical_cols)  # Set sparse=False\n",
    "    ])\n",
    "\n",
    "# Fit and transform training data\n",
    "X_train_processed = preprocessor.fit_transform(X_train)\n",
    "\n",
    "print(\"\\nProcessed training data shape:\", X_train_processed.shape)\n",
    "print(\"Type:\", type(X_train_processed))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1fee9ea",
   "metadata": {},
   "source": [
    "**Step 5: Data Preprocessing on X_test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d031b2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed test data shape: (13485, 26)\n",
      "Type: <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# Transform test data using same preprocessor\n",
    "X_test_processed = preprocessor.transform(X_test)\n",
    "\n",
    "print(\"Processed test data shape:\", X_test_processed.shape)\n",
    "print(\"Type:\", type(X_test_processed))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891f2e94",
   "metadata": {},
   "source": [
    "**Step 6: Build KNN model from scratch and predict**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "431eb03b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training KNN model...\n",
      "\n",
      "Making predictions (ultra memory efficient)...\n",
      "Processed 13000/13485 test samples...\n",
      "Sample predictions: [ 586.4 2427.6 1083.4 1196.2 9642.2]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class UltraMemoryEfficientKNN:\n",
    "    def __init__(self, k=5, train_batch_size=5000):\n",
    "        \"\"\"\n",
    "        Initialize KNN with ultra-low memory usage\n",
    "        k: number of nearest neighbors\n",
    "        train_batch_size: process training data in chunks\n",
    "        \"\"\"\n",
    "        self.k = k\n",
    "        self.train_batch_size = train_batch_size\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Store training data\"\"\"\n",
    "        self.X_train = np.array(X, dtype=np.float32)\n",
    "        self.y_train = np.array(y, dtype=np.float32)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict by processing one test sample and training batches at a time\"\"\"\n",
    "        X_test = np.array(X, dtype=np.float32)\n",
    "        n_test = X_test.shape[0]\n",
    "        n_train = self.X_train.shape[0]\n",
    "        predictions = np.zeros(n_test, dtype=np.float32)\n",
    "        \n",
    "        # Process ONE test sample at a time\n",
    "        for i in range(n_test):\n",
    "            all_distances = np.zeros(n_train, dtype=np.float32)\n",
    "            \n",
    "            # Calculate distances in training batches to save memory\n",
    "            for j in range(0, n_train, self.train_batch_size):\n",
    "                batch_end = min(j + self.train_batch_size, n_train)\n",
    "                X_train_batch = self.X_train[j:batch_end]\n",
    "                \n",
    "                # Distance for this batch: (batch_size,)\n",
    "                distances_batch = np.sqrt(\n",
    "                    np.sum((X_test[i] - X_train_batch) ** 2, axis=1)\n",
    "                )\n",
    "                all_distances[j:batch_end] = distances_batch\n",
    "            \n",
    "            # Get k nearest neighbors from all distances\n",
    "            nearest_indices = np.argpartition(all_distances, self.k)[:self.k]\n",
    "            \n",
    "            # Predict by averaging\n",
    "            predictions[i] = np.mean(self.y_train[nearest_indices])\n",
    "            \n",
    "            # Progress update\n",
    "            if (i + 1) % 500 == 0:\n",
    "                print(f\"Processed {i + 1}/{n_test} test samples...\", end='\\r')\n",
    "        \n",
    "        return predictions\n",
    "\n",
    "# --- Train and Predict ---\n",
    "print(\"Training KNN model...\")\n",
    "knn_model = UltraMemoryEfficientKNN(k=5, train_batch_size=5000)  # Adjust this lower if needed\n",
    "knn_model.fit(X_train_processed, y_train)\n",
    "\n",
    "print(\"\\nMaking predictions (ultra memory efficient)...\")\n",
    "y_pred_scratch = knn_model.predict(X_test_processed)\n",
    "\n",
    "print(\"\\nSample predictions:\", y_pred_scratch[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d249249d",
   "metadata": {},
   "source": [
    "**Step 7: Evaluate your model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4541d2d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Ultra Memory-Efficient KNN ===\n",
      "RMSE: 785.87 | MAE: 408.18 | R²: 0.9607\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred_scratch)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(y_test, y_pred_scratch)\n",
    "r2 = r2_score(y_test, y_pred_scratch)\n",
    "\n",
    "print(f\"\\n=== Ultra Memory-Efficient KNN ===\")\n",
    "print(f\"RMSE: {rmse:.2f} | MAE: {mae:.2f} | R²: {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6368fd21",
   "metadata": {},
   "source": [
    "**Step 8: Compare with sklearn KNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "24480806",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:972: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "cat_cols = [\"cut\", \"color\", \"clarity\"]\n",
    "num_cols = [c for c in X.columns if c not in cat_cols]\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    (\"cat\", OneHotEncoder(handle_unknown=\"ignore\", sparse=False), cat_cols),\n",
    "    (\"num\", StandardScaler(), num_cols)\n",
    "])\n",
    "\n",
    "X_train_p = preprocessor.fit_transform(X_train)\n",
    "X_test_p  = preprocessor.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "57723fcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scratch KNN RMSE : (computed in Step 7)\n",
      "Sklearn KNN RMSE : 1395.8127763660377\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# Apply PCA to reduce dimensionality for faster execution\n",
    "pca = PCA(n_components=5)\n",
    "\n",
    "X_train_pca = pca.fit_transform(X_train_p[:1000])\n",
    "X_test_pca  = pca.transform(X_test_p[:50])\n",
    "\n",
    "# Train sklearn KNN model\n",
    "knn_sklearn = KNeighborsRegressor(n_neighbors=3)\n",
    "y_pred_sklearn = knn_sklearn.fit(\n",
    "    X_train_pca, y_train.values[:1000]\n",
    ").predict(X_test_pca)\n",
    "\n",
    "# Calculate RMSE for sklearn KNN\n",
    "rmse_sklearn = np.sqrt(\n",
    "    mean_squared_error(y_test.values[:50], y_pred_sklearn)\n",
    ")\n",
    "\n",
    "# Comparison output\n",
    "print(\"Scratch KNN RMSE : (computed in Step 7)\")\n",
    "print(\"Sklearn KNN RMSE :\", rmse_sklearn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75f52c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
